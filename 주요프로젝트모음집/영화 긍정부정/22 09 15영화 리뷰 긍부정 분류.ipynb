{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN2/YLu2ivms69opZ0c9dbL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"YpN9wYQHdnK5"},"outputs":[],"source":["!apt-get update\n","!apt-get install g++ openjdk-8-jdk\n","!pip3 install konlpy"]},{"cell_type":"code","source":["from six.moves import urllib\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename='ratings_test.txt')"],"metadata":{"id":"8Y-dgcfeeiYt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","train_data = pd.read_table('/content/ratings_train.txt')\n","test_data = pd.read_table('/content/ratings_test.txt')"],"metadata":{"id":"ljAJ0AB3fSDq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data"],"metadata":{"id":"hmo941y2jbua"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_data"],"metadata":{"id":"1A5mLCaqjooa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_lst = train_data['document']\n","test_lst = test_data['document']"],"metadata":{"id":"1u9heDAEm70S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_lst"],"metadata":{"id":"uJAPZbiInLo5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_lst"],"metadata":{"id":"XDbfdOmV1YNj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data.info()"],"metadata":{"id":"5-KIFuMhtrDv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","sub_rext = '[^a-zA-Zㄱ-ㅣ가-힣0-9 ]'"],"metadata":{"id":"ZZIlB0xus_VP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","\n","def clean_text(texts):\n","    corpus = []\n","    for i in range(0, len(texts)):\n","        review = re.sub(sub_rext, ' ',str(texts[i])) #remove punctuation\n","        corpus.append(review)\n","    return corpus"],"metadata":{"id":"cL_qM0CIwkx-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["re_train=clean_text(train_lst)\n","re_train"],"metadata":{"id":"5x3r3GEjyTfL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["re_test=clean_text(test_lst)\n","re_test"],"metadata":{"id":"zK5Lk7lpzyV0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train = pd.DataFrame(re_train,columns = ['review'])\n","df_train"],"metadata":{"id":"n6Iw1ffF0e99"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_test = pd.DataFrame(re_test,columns = ['review'] )\n","df_test"],"metadata":{"id":"HLlTgiue0J_a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data = pd.concat([train_data, df_train], axis=1)\n","train_data"],"metadata":{"id":"9bUGKrF812mz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_data = pd.concat([test_data, df_test], axis=1)\n","test_data"],"metadata":{"id":"HAzW_osm32TJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data.drop(['document'],axis=1,inplace=True)"],"metadata":{"id":"PZLhfXf66LZh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data"],"metadata":{"id":"sjwriHxe6dmQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_data.drop(['document'],axis=1,inplace=True)"],"metadata":{"id":"mNnXzPUV6gAg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_data"],"metadata":{"id":"1BAqIElv6jWr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","myfile = files.upload()"],"metadata":{"id":"6_ZchmmzjyNq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 필요 키워드 설정\n","k_stopword = pd.read_csv('/content/korean_stopword.csv')\n","\n","stopword = list(k_stopword['불용어'])+['을','은','를','이가','과','의',\n","                                    '는','에','가','이','들','좀','잘',\n","                                    '걍','과','도','으로','자','에','와',\n","                                    '한','하다', '있다', '되다', '에서']\n","stopword[:5]"],"metadata":{"id":"-0RR9XzskF1-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from konlpy.tag import Okt\n","okt = Okt()\n","\n","morphs_lst = []\n","for i in test_data.index:\n","  morphs=\"\"\n","  morph = okt.morphs(test_data.loc[i]['review'], stem=True)\n","  for txt in morph:\n","    if txt not in stopword:\n","      morphs = morphs+\" \" +txt\n","  morphs_lst.append(morphs)\n","test_data['morphs'] = morphs_lst\n","test_data.head()"],"metadata":{"id":"RdCULdk_lDKk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from konlpy.tag import Okt\n","okt = Okt()\n","\n","morphs_lst = []\n","for i in train_data.index:\n","  morphs=\"\"\n","  morph = okt.morphs(train_data.loc[i]['review'], stem=True)\n","  for txt in morph:\n","    if txt not in stopword:\n","      morphs = morphs+\" \" +txt\n","  morphs_lst.append(morphs)\n","train_data['morphs'] = morphs_lst\n","train_data.head()"],"metadata":{"id":"Bi7COvzI6-NX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data['label'].value_counts().plot(kind = 'bar')"],"metadata":{"id":"aGtpMGOk7lZz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(train_data.isnull().values.any())"],"metadata":{"id":"BzXJeuVK9Ksn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","x = train_data['morphs']\n","y=train_data['label']\n","\n","x_train, x_test ,y_train, y_test = train_test_split(x,y,\n","                                                    test_size=0.2,\n","                                                    random_state=11)"],"metadata":{"id":"KmZv2pRg9Swt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('학습 데이터의 수: ', x_train.shape[0])\n","print('학습 데이터의 수: ', x_test.shape[0])\n","print('### 학습데이터의 라벨 분포 ###')\n","y_train.value_counts()"],"metadata":{"id":"i-cVQdF19Z4r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(x_train)"],"metadata":{"id":"TN1IUsmP9f2k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tokenizer.word_index)"],"metadata":{"id":"qgb5esBf9jvR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["threshold = 3\n","total_cnt = len(tokenizer.word_index) # 단어의 수\n","rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n","total_freq = 0# 훈련 데이터의 단어 빈도수 종합\n","rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n","\n","# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n","for key, value in tokenizer.word_counts.items():\n","  total_freq = total_freq+value\n","\n","  # 단어의 등장 빈도수가 threshold보다 작으면\n","  if(value < threshold):\n","    rare_cnt = rare_cnt+1\n","    rare_freq = rare_freq +value\n","\n","print('단어 집합(vocabulary)의 크기:',total_cnt)\n","print('등장 빈도가 %s번 이하인 희귀 단어의 수 :%s'%(threshold-1,rare_cnt))\n","print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n","print(\"전체 등장 빈도에서 희귀 단어 빈도 비율:\", (rare_freq / total_freq)*100)"],"metadata":{"id":"JkeBeknc9pz-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab_size = total_cnt - rare_cnt +2\n","print(\"단어의 집합의 크기: \",vocab_size)"],"metadata":{"id":"LsPQ87aV9wsD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = Tokenizer(vocab_size, oov_token = 'OOV')\n","tokenizer.fit_on_texts(x_train)"],"metadata":{"id":"aj2xPErh92o3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_train = tokenizer.texts_to_sequences(x_train)\n","x_test = tokenizer.texts_to_sequences(x_test)"],"metadata":{"id":"AdPDA5KO95CX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","print('문서의 최대 길이 :', max(len(l) for l in x_train))\n","print('문서의 평균 길이 :', sum(map(len, x_train))/len(x_train))\n","plt.hist([len(s) for s in x_train], bins=50)\n","plt.xlabel('length of samples')\n","plt.ylabel('number of samples')\n","plt.show()"],"metadata":{"id":"V-90hVD997Sp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def below_threshold_len(max_len, nested_list):\n","  cnt = 0\n","  for s in nested_list:\n","    if(len(s)<=max_len):\n","      cnt = cnt+1\n","  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt/len(nested_list))*100))"],"metadata":{"id":"iLa3uR6Y9-PO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_len = 35\n","below_threshold_len(max_len, x_train)"],"metadata":{"id":"uh5i6qhy-B38"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing import sequence\n","\n","print('시퀀스 패딩(sample * time)')\n","x_train = sequence.pad_sequences(x_train,maxlen=max_len)\n","x_test = sequence.pad_sequences(x_test,maxlen=max_len)\n","print('x_train 크기', x_train.shape)\n","print('x_test 크기', x_test.shape)"],"metadata":{"id":"D_yc23d9-WZd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_train, x_val, y_train, y_val = train_test_split(x_train, y_train,\n","                                                  test_size = 0.2,\n","                                                  random_state=11)"],"metadata":{"id":"ArXeIs8l-Y2G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(x_val), len(x_train))"],"metadata":{"id":"a-DUFiev-b5s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Embedding\n","\n","model = Sequential()\n","model.add(Embedding(input_dim = 1000, output_dim = 64))\n","model.add(LSTM(128, return_sequences =True))\n","model.add(LSTM(64))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(optimizer = 'adam',\n","              loss = 'binary_crossentropy',\n","              metrics = ['acc'])"],"metadata":{"id":"BmXieDcS-du8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = model.fit(x_train, y_train,\n","                    batch_size = 32, epochs=100,\n","                    validation_split=0.2)"],"metadata":{"id":"Ar8lrG38-z3S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","his_dict = history.history\n","loss = his_dict['loss']\n","val_loss = his_dict['val_loss'] \n","\n","epochs = range(1, len(loss) + 1)\n","fig = plt.figure(figsize = (10, 5))\n","\n","# 훈련 및 검증 손실 그리기\n","ax1 = fig.add_subplot(1, 2, 1)\n","ax1.plot(epochs, loss, color = 'blue', label = 'train_loss')\n","ax1.plot(epochs, val_loss, color = 'orange', label = 'val_loss')\n","ax1.set_title('train and val loss')\n","ax1.set_xlabel('epochs')\n","ax1.set_ylabel('loss')\n","ax1.legend()\n","\n","acc = his_dict['acc']\n","val_acc = his_dict['val_acc']\n","\n","# 훈련 및 검증 정확도 그리기\n","ax2 = fig.add_subplot(1, 2, 2)\n","ax2.plot(epochs, acc, color = 'blue', label = 'train_acc')\n","ax2.plot(epochs, val_acc, color = 'orange', label = 'val_acc')\n","ax2.set_title('train and val acc')\n","ax2.set_xlabel('epochs')\n","ax2.set_ylabel('acc')\n","ax2.legend()\n","\n","plt.show()"],"metadata":{"id":"TLfrVOcOWyCs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","def pred(words):\n","    words = okt.morphs(words, stem=True)\n","    words = [word for word in words if not word in stopword]\n","    encoded = tokenizer.texts_to_sequences([words])\n","    pad_words=sequence.pad_sequences(encoded,maxlen=max_len)\n","    \n","    \n","    results = model.predict(pad_words)\n","    \n","    if results >= 0.5:\n","      print('긍정')\n","    else:\n","      print('부정')"],"metadata":{"id":"8vxvIbmyZxuK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred('위성락을 뛰어넘는 또 하나의 역대급 악역이 탄생했다.')"],"metadata":{"id":"t_lWUAvkdF23"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred('빌런 진선규의 존재감은 압도적이다.')"],"metadata":{"id":"1NU1O0MZdvJf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred('너무 재미있다.')"],"metadata":{"id":"3kfi2A-Yd1VX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred('더럽게 재미없네')"],"metadata":{"id":"pjdbdGAzd4gf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred('ㅋㅋㅋ 별로임')"],"metadata":{"id":"cwajXl9Bd71g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred(' 1편보다 더 웃김ㅋㅋㅋ 배우들이 더 친해져서 그런가 확실히 더 재밌음')"],"metadata":{"id":"0mgh9zLUl7R8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred('나만 볼 수는 없지')"],"metadata":{"id":"fsDDwYhJl9tD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred('나만 볼 수 없지 ')"],"metadata":{"id":"8A8NpQITl_sX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred('ㅅㅂ 나만 볼 수 없지')"],"metadata":{"id":"RgkqW0ramC5F"},"execution_count":null,"outputs":[]}]}